# Downloaded Papers

This directory contains key research papers relevant to the hypothesis: "LLMs increases coverage not efficiency."

## Papers Summary

### 1. [Systematic Review: Developer Productivity with LLM-Assistants](2507.03156_developer_productivity_systematic_review.pdf)
- **Authors**: Multiple contributors (Systematic Literature Review)
- **Year**: 2024
- **arXiv**: 2507.03156
- **Source**: arXiv preprint
- **Why relevant**: Comprehensive review of 37 peer-reviewed studies examining LLM-assistants' impact on software developer productivity. Key findings include minimized code search, accelerated development, automation of trivial/repetitive tasks, but also concerns around cognitive offloading, reduced team collaboration, and inconsistent effects on code quality.

### 2. [The Impact of AI on Developer Productivity: Evidence from GitHub Copilot](2302.06590_copilot_productivity_impact.pdf)
- **Authors**: Peng et al.
- **Year**: 2023
- **arXiv**: 2302.06590
- **Source**: arXiv preprint
- **Why relevant**: Controlled trial with 95 professional programmers showing treated group completed tasks 55.8% faster (95% CI: 21-89%). Demonstrates empirical productivity gains but doesn't deeply analyze exploration vs efficiency trade-offs.

### 3. [Understanding the Human-LLM Dynamic: Literature Survey of LLM Use in Programming Tasks](2410.01026_human_llm_dynamic.pdf)
- **Authors**: Multiple contributors (Survey)
- **Year**: 2024
- **arXiv**: 2410.01026
- **Source**: arXiv preprint
- **Why relevant**: Identifies two main human performance metrics: time productivity and learning. Provides comprehensive view of how humans interact with LLMs in programming contexts, relevant for understanding whether efficiency gains are real or if LLMs primarily enable broader exploration.

### 4. [Evaluating the Energy-Efficiency of Code Generated by LLMs](2505.20324_energy_efficiency_code.pdf)
- **Authors**: Multiple contributors
- **Year**: 2025
- **arXiv**: 2505.20324
- **Source**: arXiv preprint
- **Why relevant**: Evaluates 20 popular LLMs showing that while LLMs produce functionally correct code, the performance and energy efficiency are often far below human-written solutions. Human-generated canonical solutions are ~1.17-2x more energy efficient than LLM-generated code, supporting the hypothesis that LLMs may not improve efficiency.

### 5. [Is LLM-Generated Code More Maintainable & Reliable than Human-Written Code?](2508.00700_maintainable_reliable_code.pdf)
- **Authors**: Multiple contributors
- **Year**: 2025
- **arXiv**: 2508.00700
- **Source**: arXiv preprint
- **Why relevant**: Empirical study comparing internal quality attributes of LLM-generated vs human-written code using SonarQube. Found that LLM-generated code has fewer bugs and requires less effort to fix them overall, but this doesn't necessarily mean higher efficiency in the development process.

### 6. [The Fault in our Stars: Quality Assessment of Code Generation Benchmarks](2404.10155_code_generation_benchmarks.pdf)
- **Authors**: Multiple contributors
- **Year**: 2024
- **arXiv**: 2404.10155
- **Source**: arXiv preprint
- **Why relevant**: First systematic study of prompt quality in code generation benchmarks, analyzing 3,566 prompts from 9 benchmarks. Identifies quality issues in datasets that could affect how we measure LLM effectiveness and whether they truly improve efficiency vs just enabling more attempts.

### 7. [Examining the Use and Impact of an AI Code Assistant on Developer Productivity and Experience](2412.06603_ai_code_assistant_enterprise.pdf)
- **Authors**: Multiple contributors (IBM Research)
- **Year**: 2024
- **arXiv**: 2412.06603
- **Source**: arXiv preprint
- **Why relevant**: Large-scale survey (105 + 564 developers) of IBM's AI assistant. Found that while it increased net productivity, gains were not evenly distributed across users. Most popular use: understanding code (71.9%) and answering programming questions (68.5%), suggesting exploration/coverage benefits.

### 8. [EVOLvE: Evaluating and Optimizing LLMs For Exploration](2410.06238_evolve_llm_exploration.pdf)
- **Authors**: Multiple contributors
- **Year**: 2024
- **arXiv**: 2410.06238
- **Source**: arXiv preprint
- **Why relevant**: Directly addresses exploration capabilities of LLMs, benchmarking them on BanditBench. Shows that fine-tuning to distill optimal exploration behavior leads to strong generalization, directly relevant to understanding whether LLMs improve exploration/coverage vs efficiency.

## Research Themes

### Coverage vs Efficiency
Papers #2, #3, #4, #7, and #8 directly speak to the core hypothesis. Evidence suggests:
- LLMs enable faster task completion (coverage of solutions)
- Energy efficiency and code quality don't always improve (efficiency concern)
- Exploration capabilities are a key benefit
- Productivity gains are inconsistent across users

### Code Quality
Papers #4, #5, and #6 examine whether LLM-generated code is actually "better":
- Functional correctness often achieved
- Energy efficiency typically worse than human code
- Maintainability shows mixed results
- Benchmark quality affects perceived performance

### Developer Experience
Papers #1, #3, and #7 examine how developers actually use LLMs:
- Primary use: understanding code and answering questions (exploration)
- Concerns about cognitive offloading
- Reduced collaboration
- Uneven productivity gains across skill levels

## Key Findings Relevant to Hypothesis

1. **Speed â‰  Efficiency**: While tasks are completed faster, the quality metrics (energy efficiency, performance) often lag behind human-written code.

2. **Exploration Emphasis**: Developers primarily use LLMs for understanding and exploration rather than direct code generation.

3. **Increased Coverage**: LLMs enable trying more approaches rapidly, consistent with "increases coverage" hypothesis.

4. **Quality Floor Raising**: LLMs help raise the lower bound of output quality in areas where they match/exceed human performance.

5. **Inconsistent Gains**: Productivity improvements are not uniform, suggesting task-dependent benefits rather than universal efficiency gains.
